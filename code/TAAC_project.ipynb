{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60104b15",
   "metadata": {
    "id": "60104b15"
   },
   "source": [
    "# Dependencies and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73d017",
   "metadata": {
    "id": "fb73d017"
   },
   "outputs": [],
   "source": [
    "%pip install -q sentence_transformers ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa87286",
   "metadata": {
    "id": "3aa87286"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Load HF_ACCESS_TOKEN from environment variable\n",
    "# Set this locally: export HF_ACCESS_TOKEN=\"your_token_here\"\n",
    "HF_ACCESS_TOKEN = os.environ.get('HF_ACCESS_TOKEN', '')\n",
    "\n",
    "TRAIN_SAMPLE_SIZE = 70\n",
    "EVAL_SAMPLE_SIZE = 30\n",
    "\n",
    "# Encoder and Retrieval (DPR) Configuration\n",
    "DPR_MODEL_NAME = 'distilbert-base-uncased' # Base model for DPR\n",
    "\n",
    "DPR_BATCH_SIZE = 8 # Batch size for DPR training\n",
    "DPR_NUM_EPOCHS = 1 # Number of epochs for DPR training\n",
    "DPR_K = 5  # Number of passages to retrieve\n",
    "\n",
    "# Note: Using distilbert-base-uncased instead of all-MiniLM-L6-v2 to avoid data leakage\n",
    "# since all-MiniLM-L6-v2 was trained on msmarco data\n",
    "\n",
    "# LLM Configuration\n",
    "LLM_MODEL_NAME = \"qwen2.5:0.5b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99cdc8",
   "metadata": {
    "id": "5a99cdc8"
   },
   "outputs": [],
   "source": [
    "OLLAMA_HOST = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05f46b",
   "metadata": {
    "id": "db05f46b"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dbe38",
   "metadata": {
    "id": "927dbe38"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def text_wrapped(text, width=150):\n",
    "    return textwrap.fill(text, width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a2040",
   "metadata": {
    "id": "336a2040"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2a5bc",
   "metadata": {
    "id": "45e2a5bc"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5f62b",
   "metadata": {
    "id": "67f5f62b"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_rag_sample(row):\n",
    "    \"\"\"\n",
    "    Extract query, positive passage, negative passages, and answers from row.\n",
    "\n",
    "    Args:\n",
    "        row (dict): A single example from the MS MARCO dataset.\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'query', 'answers', 'positive_passage', and 'negative_passages'.\n",
    "    \"\"\"\n",
    "    query = row.get(\"query\")\n",
    "    answers = row.get(\"answers\", [])\n",
    "    passages = row.get(\"passages\", {})\n",
    "\n",
    "    texts = passages.get(\"passage_text\")\n",
    "    labels = passages.get(\"is_selected\")\n",
    "\n",
    "    if not query or not texts or not labels:\n",
    "        return None\n",
    "\n",
    "    positives, negatives = [], []\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        if not text:\n",
    "            continue\n",
    "        if label == 1:\n",
    "            positives.append(text)\n",
    "        else:\n",
    "            negatives.append(text)\n",
    "\n",
    "    if len(positives) == 0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"positives\": positives,\n",
    "        \"negatives\": negatives,\n",
    "        \"answers\": answers\n",
    "    }\n",
    "\n",
    "\n",
    "def load_msmarco_dataset(sample_size=100_000):\n",
    "    \"\"\"\n",
    "    Load a finite subset of MS MARCO train split, extract RAG-ready samples\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"microsoft/ms_marco\",\n",
    "        \"v1.1\",\n",
    "        split=f\"train[:{sample_size}]\",\n",
    "    )\n",
    "\n",
    "    dataset = (\n",
    "        dataset\n",
    "        .map(extract_rag_sample)\n",
    "        .filter(lambda x: x is not None)\n",
    "        #.remove_columns(dataset.column_names)\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31b988",
   "metadata": {
    "id": "ed31b988",
    "outputId": "1f2d5ad5-46f7-4214-9eae-8b92c71b23f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 70\n",
      "Evaluation set size: 30\n"
     ]
    }
   ],
   "source": [
    "dataset = load_msmarco_dataset(sample_size=TRAIN_SAMPLE_SIZE + EVAL_SAMPLE_SIZE + 100)\n",
    "\n",
    "split_dataset = dataset.train_test_split(seed=42, test_size=EVAL_SAMPLE_SIZE, train_size=TRAIN_SAMPLE_SIZE)\n",
    "train_ds = split_dataset['train']\n",
    "eval_ds = split_dataset['test']\n",
    "\n",
    "print(f\"Training set size: {len(train_ds)}\")\n",
    "print(f\"Evaluation set size: {len(eval_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b6ece",
   "metadata": {
    "id": "881b6ece"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.select(range(TRAIN_SAMPLE_SIZE))\n",
    "eval_ds = eval_ds.select(range(EVAL_SAMPLE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c52f4b",
   "metadata": {
    "id": "21c52f4b",
    "outputId": "687603f4-76b0-49a0-e454-0633a1556005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1594\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for row in dataset:\n",
    "    corpus.extend(row[\"positives\"])\n",
    "    corpus.extend(row[\"negatives\"])\n",
    "\n",
    "corpus = list(set(corpus))\n",
    "print(f\"Corpus size: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de405a6d",
   "metadata": {
    "id": "de405a6d"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b294c93",
   "metadata": {
    "id": "0b294c93"
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5035fe",
   "metadata": {
    "id": "1a5035fe"
   },
   "outputs": [],
   "source": [
    "def prompt_with_context(query, passages):\n",
    "    context = \"\\n\\n\".join(passages)\n",
    "    return f\"\"\"You are a helpful assistant. Considering the information provided in the following context, answer the question below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77b04e",
   "metadata": {
    "id": "0e77b04e"
   },
   "outputs": [],
   "source": [
    "def prompt_no_context(query):\n",
    "    return f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824122b",
   "metadata": {
    "id": "0824122b"
   },
   "source": [
    "## Ollama Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f9a23",
   "metadata": {
    "id": "dd7f9a23",
    "outputId": "4a932e2a-b06b-4878-b269-8fc66e5fa925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%####################################################               82.6%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb7ddf",
   "metadata": {
    "id": "99fb7ddf"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  \"\"\"\n",
    "  Run ollama serve in a separate thread.\n",
    "  \"\"\"\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4846cfd",
   "metadata": {
    "id": "d4846cfd"
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def call_ollama(prompt, model=LLM_MODEL_NAME, verbose=False):\n",
    "    \"\"\"\n",
    "    Call the Ollama LLM with the given prompt and model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM.\n",
    "        model (str): The name of the LLM model to use.\n",
    "        verbose (bool): Whether to display the prompt and response.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the LLM's response.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    },\n",
    "    ])\n",
    "\n",
    "    if verbose:\n",
    "        display(Markdown(f\"**Prompt**:\\n '{prompt}'\"))\n",
    "        display(Markdown(text_wrapped(f\"**Ollama response:**\\n '{response['message']['content']}'\")))\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04893bbe",
   "metadata": {
    "id": "04893bbe",
    "outputId": "369ac9cd-1cf8-492c-bc3e-23363c31f5a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb9eac",
   "metadata": {
    "id": "29fb9eac"
   },
   "source": [
    "Let's check if we can prompt the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ec762",
   "metadata": {
    "id": "a45ec762",
    "outputId": "3079243d-298c-4756-9209-c338b6fa0256"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hello Ollama! I'm Qwen, an artificial intelligence language model designed to assist you with various tasks and inquiries. How can I help you today? Is there anything specific you would like to discuss or ask about?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_ollama(\"hello, ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a51fd",
   "metadata": {
    "id": "d89a51fd"
   },
   "source": [
    "## RAG with Dense Passage Retrieval (DPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc753a",
   "metadata": {
    "id": "cbbc753a"
   },
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0abca1",
   "metadata": {
    "id": "3e0abca1"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "def build_dpr_training_data(dataset, n_negatives=10):\n",
    "    \"\"\"\n",
    "    Build DPR training data with one positive and n_negatives negatives per query (1-1-N).\n",
    "\n",
    "    It uses Truncation & Pad strategy for negatives, so all negatives have the same length.\n",
    "    - When row has n_negatives negatives, they will all be used.\n",
    "    - When row has more than n_negatives negatives, sample n_negatives of them.\n",
    "    - When row has fewer than n_negatives negatives, use all negatives and sample the rest with replacement.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset with 'query', 'positives', and 'negatives' fields.\n",
    "        n_negatives: Number of negatives to use per query.\n",
    "\n",
    "    Returns:\n",
    "        List of InputExample objects for training.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for row in dataset:\n",
    "        pos = row[\"positives\"][0]  # use only one positive passage\n",
    "        negatives = row[\"negatives\"] # must have n_negatives negatives\n",
    "\n",
    "        if len(negatives) >= n_negatives:\n",
    "            # Standard sampling (no duplicates needed)\n",
    "            negatives = random.sample(negatives, k=n_negatives)\n",
    "        else:\n",
    "            # We need more negatives than we have, so use random.choices which allows duplication\n",
    "            n_to_sample = n_negatives - len(negatives)\n",
    "            neg_samples = random.choices(negatives, k=n_to_sample)\n",
    "            negatives.extend(neg_samples)\n",
    "        examples.append(InputExample(texts=[row[\"query\"], pos, *negatives]))\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39bda4",
   "metadata": {
    "id": "9d39bda4"
   },
   "source": [
    "### Model Implementation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786bede",
   "metadata": {
    "id": "7786bede"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import os\n",
    "\n",
    "def build_dpr_model(model_name):\n",
    "    \"\"\"\n",
    "    Build a DPR model with a transformer and mean pooling.\n",
    "    Args:\n",
    "        model_name (str): Name of the transformer model to use.\n",
    "    Returns:\n",
    "        SentenceTransformer: The constructed DPR model.\n",
    "    \"\"\"\n",
    "    word_embedding_model = models.Transformer(\n",
    "        model_name,\n",
    "        max_seq_length=256\n",
    "    )\n",
    "\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True\n",
    "    )\n",
    "\n",
    "    return SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c664bc",
   "metadata": {
    "id": "c7c664bc"
   },
   "outputs": [],
   "source": [
    "# Model & loss\n",
    "retriever = build_dpr_model(DPR_MODEL_NAME)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e19d3d",
   "metadata": {
    "id": "98e19d3d"
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_examples = build_dpr_training_data(train_ds)\n",
    "train_loader = DataLoader(train_examples, batch_size=DPR_BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9ce06",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9322e4f72b134783988380f39ce8429b"
     ]
    },
    "id": "09e9ce06",
    "outputId": "85161a56-fe1a-45af-cb1d-68d4a1c0d9e4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9322e4f72b134783988380f39ce8429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit\n",
    "retriever.fit(\n",
    "    train_objectives=[(train_loader, train_loss)],\n",
    "    epochs=DPR_NUM_EPOCHS,\n",
    "    warmup_steps=int(len(train_loader) * 0.1),\n",
    "    output_path=\"./dpr_retriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948602b",
   "metadata": {
    "id": "f948602b"
   },
   "source": [
    "### Encoding and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47460722",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "58567cbe296942acaef713b7f8f93933"
     ]
    },
    "id": "47460722",
    "outputId": "a3e7e2a1-d5e4-491e-e4d7-b0167eab406a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58567cbe296942acaef713b7f8f93933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_corpus(corpus, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Encode a passage corpus using the given model.\n",
    "\n",
    "    Args:\n",
    "        corpus: List of passage texts to encode.\n",
    "        model: SentenceTransformer model to use for encoding.\n",
    "        batch_size: Batch size for encoding.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of encoded passage embeddings.\n",
    "    \"\"\"\n",
    "    return model.encode(\n",
    "        corpus,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "corpus_embeddings = encode_corpus(corpus, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78cff8",
   "metadata": {
    "id": "ae78cff8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dpr_retrieve(query, corpus, corpus_embeddings, model, k=10, with_scores=False):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages from the corpus for the given query using DPR.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "        corpus (list): List of passage texts in the corpus.\n",
    "        corpus_embeddings (Tensor): Precomputed embeddings of the corpus passages.\n",
    "        model: SentenceTransformer model used for encoding.\n",
    "        k (int): Number of top passages to retrieve.\n",
    "        with_scores (bool): Whether to return scores along with passages.\n",
    "    Returns:\n",
    "        list: Top-k retrieved passages (and scores if with_scores is True).\n",
    "    \"\"\"\n",
    "\n",
    "    q_emb = model.encode(query, convert_to_tensor=True)\n",
    "    scores = torch.matmul(corpus_embeddings, q_emb)\n",
    "    top_k = torch.topk(scores, k=k)\n",
    "\n",
    "    indices = top_k.indices.tolist()\n",
    "\n",
    "    if not with_scores:\n",
    "        return [corpus[i] for i in indices]\n",
    "    else:\n",
    "        values = top_k.values.tolist()\n",
    "        return [(corpus[i], values[j]) for j, i in enumerate(indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9448d2",
   "metadata": {
    "id": "ac9448d2"
   },
   "source": [
    "### Evaluation with Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95948ae3",
   "metadata": {
    "id": "95948ae3"
   },
   "outputs": [],
   "source": [
    "def reciprocal_rank(retrieved_passages, positive_set):\n",
    "    \"\"\"\n",
    "    Calculate the Reciprocal Rank (RR) for a single query.\n",
    "\n",
    "    Args:\n",
    "        retrieved_passages (list): List of tuples (passage, score) retrieved for the query.\n",
    "        positive_set (set): Set of positive passages for the query.\n",
    "    Returns:\n",
    "        float: The Reciprocal Rank value.\n",
    "    \"\"\"\n",
    "\n",
    "    for rank, (passage, _) in enumerate(retrieved_passages, start=1):\n",
    "        if passage in positive_set:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_mrr(dataset, retriever, corpus, k=10, n_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate Mean Reciprocal Rank (MRR) over the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset with 'query' and 'positives' fields.\n",
    "        retriever: SentenceTransformer model used for retrieval.\n",
    "        corpus: List of passage texts in the corpus.\n",
    "        k (int): Number of top passages to retrieve.\n",
    "        n_samples (int): Number of samples from the dataset to evaluate.\n",
    "    Returns:\n",
    "        float: The Mean Reciprocal Rank (MRR) value.\n",
    "    \"\"\"\n",
    "\n",
    "    rrs = []\n",
    "\n",
    "    for sample in dataset.select(range(n_samples)):\n",
    "        positives = set(sample[\"positives\"])\n",
    "\n",
    "        retrieved = dpr_retrieve(\n",
    "            sample[\"query\"],\n",
    "            corpus,\n",
    "            corpus_embeddings,\n",
    "            retriever,\n",
    "            k=k,\n",
    "            with_scores=True\n",
    "        )\n",
    "\n",
    "        rr = reciprocal_rank(retrieved, positives)\n",
    "        rrs.append(rr)\n",
    "\n",
    "    return sum(rrs) / len(rrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb629b58",
   "metadata": {
    "id": "fb629b58"
   },
   "source": [
    "Let's see how MRR operates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd66c0",
   "metadata": {
    "id": "32fd66c0",
    "outputId": "6b3d95c8-1b82-4c48-899f-afb69e08fc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@5 over 30 samples: 0.1961\n",
      "MRR@15 over 30 samples: 0.2200\n",
      "MRR@30 over 30 samples: 0.2264\n"
     ]
    }
   ],
   "source": [
    "mrr_score_5 = evaluate_mrr(eval_ds, retriever, corpus, k=5, n_samples=EVAL_SAMPLE_SIZE)\n",
    "print(f\"MRR@5 over {EVAL_SAMPLE_SIZE} samples: {mrr_score_5:.4f}\")\n",
    "\n",
    "mrr_score_15 = evaluate_mrr(eval_ds, retriever, corpus, k=15, n_samples=EVAL_SAMPLE_SIZE)\n",
    "print(f\"MRR@15 over {EVAL_SAMPLE_SIZE} samples: {mrr_score_15:.4f}\")\n",
    "\n",
    "mrr_score_30 = evaluate_mrr(eval_ds, retriever, corpus, k=30, n_samples=EVAL_SAMPLE_SIZE)\n",
    "print(f\"MRR@30 over {EVAL_SAMPLE_SIZE} samples: {mrr_score_30:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52b021",
   "metadata": {
    "id": "0e52b021"
   },
   "source": [
    "We can notice that, as we increase number of K retrieved, the MRR also increases. When we increase k, we are widening the number of passages retrieved, so the probability of retrieving a relevant passage increase. Therefore, a document that was \"missing\" at k=5 might be found at e.g. k=12. This changes that specific query's score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gc7sz8fQRdcu",
   "metadata": {
    "id": "Gc7sz8fQRdcu"
   },
   "source": [
    "# (WIP) Custom Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bf73b",
   "metadata": {
    "id": "7c9bf73b"
   },
   "source": [
    "## Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d5799",
   "metadata": {
    "id": "001d5799"
   },
   "outputs": [],
   "source": [
    "def run_baseline(sample, verbose=False):\n",
    "    \"\"\"\n",
    "    Run baseline LLM without context.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A sample containing the 'query'.\n",
    "        verbose (bool): Whether to display prompt and response.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response.\n",
    "    \"\"\"\n",
    "    prompt = prompt_no_context(sample[\"query\"])\n",
    "    return call_ollama(prompt, verbose=verbose)\n",
    "\n",
    "\n",
    "def run_oracle(sample, k=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Run oracle LLM with ground-truth context.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A sample containing the 'query' and 'positives'.\n",
    "        k (int): Number of ground-truth passages to use.\n",
    "        verbose (bool): Whether to display prompt and response.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response.\n",
    "    \"\"\"\n",
    "    gold_context = sample[\"positives\"][:k]\n",
    "    prompt = prompt_with_context(sample[\"query\"], gold_context)\n",
    "    return call_ollama(prompt, verbose=verbose)\n",
    "\n",
    "\n",
    "def run_rag_dpr(sample, retriever, corpus, k=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Run RAG with DPR retrieval.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A sample containing the 'query'.\n",
    "        retriever: The DPR retriever model.\n",
    "        corpus (list): The passage corpus.\n",
    "        k (int): Number of passages to retrieve.\n",
    "        verbose (bool): Whether to display prompt and response.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response.\n",
    "    \"\"\"\n",
    "    retrieved_context = dpr_retrieve(\n",
    "        query=sample[\"query\"],\n",
    "        corpus=corpus,\n",
    "        corpus_embeddings=corpus_embeddings,\n",
    "        model=retriever,\n",
    "        k=k\n",
    "    )\n",
    "    prompt = prompt_with_context(sample[\"query\"], retrieved_context)\n",
    "\n",
    "    return call_ollama(prompt, verbose=verbose)\n",
    "\n",
    "\n",
    "def run_rag_dpr_reranker(sample):\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6ee76",
   "metadata": {
    "id": "5ff6ee76"
   },
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "el4KEtL8RQ23",
   "metadata": {
    "id": "el4KEtL8RQ23"
   },
   "source": [
    "## Run with single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45fb9c",
   "metadata": {
    "id": "5a45fb9c"
   },
   "source": [
    "First, let's check if approaches work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab3466",
   "metadata": {
    "id": "0bab3466"
   },
   "outputs": [],
   "source": [
    "sample = train_ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb31d7",
   "metadata": {
    "id": "b2fb31d7",
    "outputId": "501db818-76ad-4cf3-b926-cae1b36ba0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt**:\n",
       " 'You are a helpful assistant.\n",
       "\n",
       "Question:\n",
       "cortical functions of the brain\n",
       "\n",
       "Answer:\n",
       "'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Ollama response:**  'Cortical functions of the brain refer to the functions that are primarily associated with neural structures in the cortex,\n",
       "which is the outermost layer of the cerebral hemispheres (left and right sides). Cerebral cortex is responsible for processing sensory information,\n",
       "controlling movement, and controlling motor responses. Some key features of cortical functions include:  1. Processing visual information: The primary\n",
       "function of the visual cortex is to process visual stimuli from the outside world.  2. Motor control: The somatosensory cortex plays a crucial role in\n",
       "regulating muscle movements based on sensory feedback from the body.  3. Language processing: Broca's area and Wernicke's area are involved in\n",
       "language production and comprehension, respectively.  4. Emotional regulation: Areas like the anterior cingulate cortex and amygdala play important\n",
       "roles in emotional responses and processing.  5. Motor learning: The basal ganglia, particularly the caudate nucleus, is linked to cognitive control\n",
       "through the striatum and the medial prefrontal cortex.  6. Pain perception: The pain threshold region of the parieto-occipital cortex controls our\n",
       "perception of pain.  7. Sensory integration: The primary somatosensory cortex in humans assists in integrating sensory information from different\n",
       "parts of the body, which is essential for fine motor skills and spatial orientation.  8. Visual field processing: Damage to the occipito-temporal\n",
       "cortex can affect visual processing abilities.  9. Pain perception: Many areas of the cerebral cortex are involved in pain sensations, including the\n",
       "primary somatosensory cortex and the brainstem.  10. Sensory integration: The parieto-occipital cortex is responsible for integrating sensory\n",
       "information from different parts of the body to form a coherent perception.  Cortical functions are further subdivided into several subcategories such\n",
       "as language areas, motor regions, visual fields, pain processing, and sensory integration. Understanding these cortical functions can provide valuable\n",
       "insights into brain function and how various aspects of cognition and behavior interact with each other.'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORACLE ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt**:\n",
       " 'You are a helpful assistant. Considering the information provided in the following context, answer the question below.\n",
       "\n",
       "Context:\n",
       "Cerebral Cortex: The cerebral cortex covers the outer portion (1.5mm to 5mm) of the cerebrum. It is the layer of the brain often referred to as gray matter. The cortex (thin layer of tissue) is gray because nerves in this area lack the insulation that makes most other parts of the brain appear to be white. Most information processing occurs in the cerebral cortex. The cerebral cortex is divided into lobes that each have a specific function. Function: The cerebral cortex is involved in several functions of the body including: 1  Determining Intelligence. 2  Determining Personality. 3  Motor Function. 4  Planning and Organization.\n",
       "\n",
       "Question:\n",
       "cortical functions of the brain\n",
       "\n",
       "Answer:\n",
       "'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Ollama response:**  'Based on the provided context, the cortical functions of the brain include:  1. Determining Intelligence. 2. Determining\n",
       "Personality. 3. Motor Function. 4. Planning and Organization.  These functions are often referred to as \"cortex-related functions\" or \"cortex-related\n",
       "processes,\" indicating their involvement in how the brain processes information related to these cognitive domains.'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG DPR ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt**:\n",
       " 'You are a helpful assistant. Considering the information provided in the following context, answer the question below.\n",
       "\n",
       "Context:\n",
       "Introduction. The forebrain (proencephalon) is the largest part of the brain, most of which is cerebrum. Other important structures found in the forebrain include the thalamus, the hypothalamus and the limbic system. Limbic System. The Limbic system is made up of parts of the brain bordering the corpus collosum. The Limbic system contains areas of cerebral cortex, the cingulate gyrus (dorsally), the parahippocampus gyrus (ventrally), the amygdala, parts of the hypothalamus (mamillary body) and the hippocampus.\n",
       "\n",
       "Dopamine production. Dopamine is produced in several areas of the brain, including the substantia nigra and the ventral tegmental area. It is a neurohormone that is released by the hypothalamus.\n",
       "\n",
       "The subcortical structures of the forebrain are located deeper in the brain and include the basal ganglia, hippocampus and amygdala. The basal ganglia, groups of nerve cells, are responsible for the coordination of movement. Both the hippocampus and amygdala are parts of the limbic system. The cerebrum includes the cerebral cortex, which is made up of four lobes: frontal, temporal, parietal and occipital. Imagining, planning and reasoning are some of the functions of the frontal lobe. Hearing and smelling, as well as forming memories and retrieving them later, are functions of the temporal lobes.\n",
       "\n",
       "The brain is made of three main parts: the forebrain, midbrain, and hindbrain. The forebrain consists of the cerebrum, thalamus, and hypothalamus (part of the limbic system). The midbrain consists of the tectum and tegmentum. The hindbrain is made of the cerebellum, pons and medulla. Often the midbrain, pons, and medulla are referred to together as the brainstem\n",
       "\n",
       "The Cerebrum: The cerebrum or cortex is the largest part of the human brain, associated with higher brain function such as thought and action. The cerebral cortex is divided into four sections, called lobes: the frontal lobe, parietal lobe, occipital lobe, and temporal lobe. The forebrain consists of the cerebrum, thalamus, and hypothalamus (part of the limbic system). The midbrain consists of the tectum and tegmentum. The hindbrain is made of the cerebellum, pons and medulla.\n",
       "\n",
       "Question:\n",
       "cortical functions of the brain\n",
       "\n",
       "Answer:\n",
       "'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Ollama response:**  'The corticofugal function refers to the cerebral cortex being associated with higher brain function such as thought and action.\n",
       "It is made up of four sections known as lobes: the frontal lobe, parietal lobe, occipital lobe, and temporal lobe.  The hypothalamus plays a\n",
       "significant role in regulating various bodily functions, including temperature, sleep patterns, hunger, thirst, sexual arousal, body temperature\n",
       "regulation, water intake, etc. It also affects emotions and behavior. Additionally, the thalamus is involved in processing sensory information from\n",
       "the external environment, such as sound, touch, taste, sight, pain, etc., to form cognitive and motor responses.  The limbic system is a complex\n",
       "network of brain structures that includes parts responsible for storing and regulating emotional memories, behaviors, feelings, emotions, arousal,\n",
       "perception, learning, motivation, and social interactions. The amygdala, part of the hippocampus and hypothalamus, plays important roles in processing\n",
       "negative stimuli (such as fear) and managing stress responses.  The basal ganglia are a group of nuclei located within the cerebrum that play a\n",
       "crucial role in motor control, behavior modification and cognitive functions. They also play a significant role in regulating attention, motivation,\n",
       "learning, social behaviors, reward, reinforcement, decision making and sensory processing.'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== BASELINE ===\")\n",
    "response_baseline = run_baseline(sample, verbose=True)\n",
    "\n",
    "print(\"\\n=== ORACLE ===\")\n",
    "response_upperline = run_oracle(sample, verbose=True)\n",
    "\n",
    "print(\"\\n=== RAG DPR ===\")\n",
    "response_rag_dpr = run_rag_dpr(sample, retriever, corpus, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cc6b7",
   "metadata": {
    "id": "481cc6b7"
   },
   "source": [
    "## (WIP) Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e7fdf",
   "metadata": {
    "id": "dc0e7fdf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_methods(\n",
    "    dataset,\n",
    "    retriever,\n",
    "    corpus,\n",
    "    n_samples=100\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for sample in dataset.select(range(n_samples)):\n",
    "        gold_answers = sample[\"answers\"]\n",
    "\n",
    "        baseline_out = run_baseline(sample)\n",
    "        dpr_out = run_rag_dpr(sample, retriever, corpus)\n",
    "        upper_out = run_oracle(sample)\n",
    "\n",
    "        results.append({\n",
    "            \"query\": sample[\"query\"],\n",
    "            \"gold_answers\": gold_answers,\n",
    "            \"baseline\": baseline_out,\n",
    "            \"rag_dpr\": dpr_out,\n",
    "            \"upperline\": upper_out\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38722a1",
   "metadata": {
    "id": "e38722a1"
   },
   "outputs": [],
   "source": [
    "EVAL_SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51709f76",
   "metadata": {
    "id": "51709f76"
   },
   "outputs": [],
   "source": [
    "results = evaluate_methods(\n",
    "    dataset=eval_ds,\n",
    "    retriever=retriever,\n",
    "    corpus=corpus,\n",
    "    n_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd293f",
   "metadata": {
    "id": "41dd293f"
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1194d55",
   "metadata": {
    "id": "b1194d55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
