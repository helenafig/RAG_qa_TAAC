\documentclass[twocolumn,11pt]{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{geometry}
\usepackage[hidelinks]{hyperref}

\geometry{margin=0.75in, top=1in}

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand\confname{Advanced Topics on Machine Learning}
\newcommand\confyear{2025/2026}

\title{Document Information as Non-Parametric Memory using Dense Passage
  Retrieval and Reranker Integrated to a LLM}

\author{Danillo Silva \and Helena Alves \and Paula Ito \\
MSc Data Science and Engineering \\
Faculty of Engineering, University of Porto (FEUP) \\
Portugal}

\date{December 2025}

\setlength{\parskip}{0pt}
\setlength{\parindent}{1em}

\begin{document}
\maketitle

\begin{abstract}
Large language models (LLMs) may produce answers that appear correct but lack
factual grounding, particularly in scientific domains. This work implements and
evaluates a Retrieval-Augmented Generation (RAG) pipeline for scientific
fact-checking using the SciFact dataset. The pipeline integrates Dense Passage
Retrieval (DPR) with a cross-encoder reranker to improve evidence selection
quality. Four approaches are compared: baseline LLM-only, upperline LLM-only with golden
context, RAG with DPR, and RAG with DPR plus reranking. Results demonstrate
that the two-stage retrieval strategy (bi-encoder followed by cross-encoder
reranking) improves retrieval precision and retrieval quality, with reranking
achieving higher Mean Reciprocal Rank (MRR) across configurations. While
retrieval-augmented approaches improve factual grounding compared to the
baseline, performance remains sensitive to retrieval errors, highlighting the
importance of robust evidence selection for fact-checking tasks.
\end{abstract}

\section{Introduction}

Large language models have demonstrated remarkable capabilities in answering
questions and reasoning tasks, yet they suffer from fundamental limitations in
knowledge-intensive domains. Their training data becomes outdated, and they
cannot reliably cite sources for factual claims, leading to hallucination
problems especially pronounced in scientific and specialized domains where
accuracy is critical.

Recent work has addressed these limitations through Retrieval-Augmented
Generation (RAG), which combines neural generation with external information
retrieval to ground model outputs in retrievable evidence
(\cite{lewis2020retrieval}). Dense Passage Retrieval (DPR) provides an effective
retrieval mechanism using learned dense representations (\cite{karpukhin2020dense}),
though bi-encoder architectures may still retrieve semantically similar but
factually incorrect passages. Cross-encoder reranking addresses this by jointly
modeling query-passage interactions to refine candidate rankings
(\cite{karpukhin2020dense}).

Although most RAG work focuses on open-domain question answering, this project
applies RAG to scientific fact-checking, where assertions must be verified
against scientific evidence. Given a scientific claim, the system must
determine whether supporting or contradicting evidence exists in a document
corpus. This task demands both effective retrieval and robust evidence-based
reasoning.

\subsection{Problem Statement and Objectives}

The goal of this project was to implement a complete RAG pipeline for
scientific fact-checking and evaluate the impact of dense retrieval DPR and
reranking on both retrieval quality (Mean Reciprocal Rank) and end-to-end
fact-checking accuracy.
Specifically, the study compare four approaches:

\begin{enumerate}
    \item Baseline: using LLM-only.
    \item Upperline (Oracle): LLM provided with golden context
    \item RAG with DPR: LLM provided with context retrieved by DPR
    \item RAG with DPR and Reranker: LLM provided with context reranked after DPR candidate selection
\end{enumerate}

\subsection{Dataset and Key Concepts}

The study uses the SciFact dataset~(\cite{wadden2020fact}), publicly available on
Hugging Face (bigbio/scifact: scifact\_labelprediction\_bigbio\_pairs subset).
Each example contains a claim, a passage, and a label: SUPPORT, CONTRADICT, or
NOINFO.
For our training setup, SUPPORT and CONTRADICT passages are treated as
positives, while NOINFO passages serve as negatives.
The dataset is split into training and evaluation sets as follows: training set
with 594 claims, evaluation set with 188 claims, and corpus size of 2,263
passages. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{viz/dataset.jpg}
    \caption{SciFact dataset raw schema and after data preprocessing}
    \label{fig:dataset}
\end{figure}

\section{Related Work}

\textbf{Retrieval-Augmented Generation (RAG).}
Lewis et al.~\cite{lewis2020retrieval} combine parametric generation with
non-parametric memory through retrieval, reducing hallucinations and improving
factuality for knowledge-intensive tasks. RAG enables knowledge updates by
modifying the retrieval index without retraining and has demonstrated effectiveness
in open-domain QA and fact verification.

\textbf{Dense Passage Retrieval (DPR).}
Karpukhin et al.~\cite{karpukhin2020dense} introduce DPR, a dual-encoder
framework trained with contrastive learning that captures semantic similarity
between queries and passages. DPR outperforms sparse methods like BM25 by
leveraging dense representations and enables efficient retrieval through
precomputed passage embeddings and approximate nearest neighbor search.

\textbf{Cross-Encoder Reranking.}
Cross-encoders jointly encode query-passage pairs to compute precise relevance
scores, improving upon bi-encoders by modeling fine-grained query-passage
interactions \cite{karpukhin2020dense}. In fact-checking, reranking is
particularly valuable for distinguishing semantically relevant but factually
incorrect passages from genuine supporting evidence. The two-stage architecture
(recall-focused bi-encoder followed by precision-focused cross-encoder) improves
retrieval quality without significant computational overhead.

\textbf{Efficient Transformers (DistilBERT).}
DistilBERT \cite{sanh2020distilbert} is a distilled version of BERT
\cite{devlin2018bert} that retains 97\% of performance while being 40\% smaller
and 60\% faster. Its efficiency makes it suitable for resource-constrained
retrieval systems. In this work, DistilBERT serves as the backbone encoder for
both DPR and the reranker.

\section{Methodology}

This section describes the technical approach to implementing the RAG pipeline
for scientific fact-checking. The methodology covers: (1) input preprocessing and
tokenization using DistilBERT, (2) training and deployment of the DPR retriever
with contrastive learning, (3) development of a cross-encoder reranker using
hard negative mining, (4) the four computational approaches evaluated for
fact-checking (baseline, oracle, RAG with DPR, and RAG with DPR + reranker),
and (5) evaluation metrics for both retrieval quality and end-to-end accuracy.

\subsection{Preprocessing and Tokenization}

Input sequences are tokenized using the \texttt{distilbert-base-uncased}
tokenizer with a maximum length of 256 tokens. Variable-length sequences are
padded to this maximum length. Attention masks are generated to distinguish real
tokens (mask value 1) from padding tokens (mask value 0), ensuring that padding
tokens do not influence self-attention computations.

\subsection{Dense Passage Retrieval (DPR) Model}

The DPR system employs a bi-encoder architecture consisting of a query encoder
and a passage encoder, both implemented using DistilBERT. The query encoder
encodes claims into dense vectors, while the passage encoder independently
encodes candidate passages into corresponding dense vectors. Mean pooling over
token embeddings produces fixed-size vector representations. Similarity scoring
is computed using batch dot-product operations between query and passage
embeddings.

Training uses a 1--1--N sampling strategy: each sample comprises one positive
passage paired with 10 negative passages. Query and passage embeddings are
computed separately, then passage embeddings are grouped with their
corresponding query as (batch size, 1 + N negatives, embedding dimension).
Similarity scores are computed via batch matrix multiplication, producing scores
for the positive and 10 negatives. The training objective treats this as a
classification problem using CrossEntropyLoss, where the target is always index 0
(the positive passage), forcing the model to assign the highest score to the
positive while penalizing high scores for negatives. The model is trained for 2
epochs with a learning rate of $2\times10^{-5}$, weight decay of 0.01, using the
AdamW optimizer. At inference, all corpus passages are pre-encoded and indexed,
enabling fast retrieval of top-$K$ candidates through efficient nearest neighbor
search.

\subsection{Cross-Encoder Reranker}

The reranking component is implemented as a DistilBERT-based cross-encoder that
differs fundamentally from the bi-encoder architecture. While DPR encodes queries
and passages independently, the cross-encoder jointly tokenizes and encodes
query-passage pairs, enabling the model to attend across the full sequence and
capture fine-grained semantic interactions. The \texttt{[CLS]} token embedding
serves as an aggregate representation, followed by dropout (rate 0.1) for
regularization and a linear scoring head that produces a relevance logit.

Hard negative mining is central to reranker training. The trained DPR retriever
is used to retrieve top-$K$ passages for each training claim (retrieval with
k = N negatives + 5 to account for potential overlap). Passages retrieved with
high DPR similarity that do not match the ground-truth positive are selected as
hard negatives. This strategy ensures the reranker learns to distinguish between
semantically similar passages and genuinely relevant evidence, which is critical
for fact-checking where a passage may discuss the topic but contradict or fail
to support the claim.

Training data consists of (query, positive, negative 1, \ldots, negative $N$)
tuples. The cross-encoder processes each query-passage pair jointly: queries and
passages are tokenized together with automatic padding and truncation at the
batch level. The model is trained for 3 epochs using Binary Cross-Entropy with
Logits loss with class weighting (pos\_weight = $N$ negatives) to address the
imbalance of one positive per $N$ negatives. The optimizer is AdamW with
learning rate $2\times10^{-5}$ and weight decay 0.01.

\subsection{Experiment Pipeline}

Four computational approaches are evaluated for scientific fact-checking:

\textbf{Approach 1: Baseline (LLM-only).} The baseline system prompts the
language model with only the claim text, without external evidence. The model
outputs a JSON object containing a \texttt{label} field (True/False) and a
\texttt{reasoning} field providing brief justification.

\textbf{Approach 2: Oracle (LLM + Golden Context).} The oracle configuration
supplies the language model with the gold-standard evidence passages
corresponding to the claim. This approach establishes an upper bound on
achievable performance assuming perfect retrieval.

\textbf{Approach 3: RAG with DPR.} Candidate passages are retrieved using the
DPR model, and the top-$K$ passages are provided as context to the language
model for fact-checking.

\textbf{Approach 4: RAG with DPR + Reranker.} An initial candidate pool is
retrieved using DPR, then reranked using the cross-encoder. The top-$K$
reranked passages are provided as context to the language model.

The language model component utilizes Qwen 2.5 7B (quantized) served locally via
Ollama. One-shot chain-of-thought prompting is applied, with the prompt
structure varying by approach: the baseline prompt contains only the claim,
while oracle and RAG approaches include retrieved evidence as context. The model
outputs a JSON object with \texttt{label} and \texttt{reasoning} fields for all
approaches.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{./viz/approaches.jpg}
\caption{Overview of the four approaches compared in this study: (1) baseline LLM-only, (2) oracle with golden context, (3) RAG with DPR, and (4) RAG with DPR plus reranker.}
\label{fig:approaches}
\end{figure}

\subsection{Evaluation Metrics}

\textbf{Retrieval quality:} assessed using Mean Reciprocal Rank (MRR). For each query,
the reciprocal of the rank of the first relevant passage is computed, and MRR is
the average of these values across all queries. MRR is evaluated under different
candidate pool sizes and context depths.

\textbf{End-to-end fact-checking performance:} measured using accuracy, computed as the
proportion of predictions matching the ground-truth labels from the SciFact
dataset.

\section{Results}

This section presents both qualitative and quantitative results. We first analyze example outputs, then report retrieval metrics and end-to-end accuracy.

\subsection{Qualitative Examples}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{viz/llm_retrieval_example_1.jpg}
\caption{Outputs for a claim under baseline, oracle, DPR, and DPR+reranker.
  The baseline relies only on parametric knowledge and may hallucinate. Oracle
  uses gold evidence. DPR retrieval improves grounding, and reranking provides
  more relevant evidence, improving reasoning quality.}
\label{fig:ex1}
\end{figure}

Outputs displayed in Figure~\ref{fig:ex1} highlights the main behavior observed
across the dataset. The baseline sometimes produces confident answers with weak
evidence. When correct passages are provided (oracle), the reasoning becomes
more detailed and aligned with evidence. In the RAG settings, the quality of the
retrieved context directly affects correctness: when retrieval includes relevant
evidence, the model's reasoning becomes more concrete; when retrieval includes
partially relevant or misleading passages, the model can be pushed toward the
wrong label.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{viz/llm_retrieval_example_2.jpg}
\caption{Outputs for a claim about CO\textsubscript{2} reduction. The baseline
  may guess correctly but with generic reasoning. RAG improves evidence-based
  explanation, especially when reranking selects passages closer to the claim
  semantics.}
\label{fig:ex2}
\end{figure}

In Figure~\ref{fig:ex2} outputs, it shows that even when the baseline predicts
correctly, it often produces generic reasoning. With RAG, the reasoning is more
grounded, because the model can point to retrieved sentences. This supports the
core motivation of RAG: reducing hallucination and improving transparency
\cite{lewis2020retrieval}.

\subsection{Retrieval Performance (MRR)}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{viz/mrr_results.png}
\caption{MRR results for DPR vs.\ DPR with reranker under different candidate
  pool sizes and context depths (3, 5, 10). Reranking consistently increases MRR
  across settings.}
\label{fig:mrr1}
\end{figure}

Figure~\ref{fig:mrr1} shows that DPR with reranking achieves higher MRR than DPR
alone for all candidate pool sizes. This is expected: the DPR bi-encoder is
efficient but can retrieve semantically similar passages that are not the best
evidence. The reranker (cross-encoder) captures richer interactions between claim
and passage and improves ranking.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{viz/mrr_dpr_vs_dpr_reranker.png}
\caption{Left: DPR-only MRR increases with context size. Right: reranking
  improves MRR further as candidate pool increases. Larger candidate pools allow
  better reranking but increase computation.}
\label{fig:mrr2}
\end{figure}

Figure~\ref{fig:mrr2} helps interpret the trend: increasing the number of
candidates generally increases MRR with reranking because the reranker has more
options to choose from. This matches the standard view that retrieval is a
recall-focused step and reranking is a precision-focused step.

\subsection{End-to-End Accuracy}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{viz/llm_accuracy.png}
\caption{Fact-checking accuracy by approach and retrieval configuration. Oracle
  is the upper bound. RAG improves over baseline when retrieved evidence is
  correct; reranking helps by selecting better contexts.}
\label{fig:acc}
\end{figure}

Figure~\ref{fig:acc} shows that the oracle configuration achieves the highest
accuracy, confirming that correct evidence leads to strong downstream
performance. The baseline performance is relatively high, suggesting that the LLM
has strong general scientific knowledge, but it is not always reliable. The RAG
approaches improve grounding, but they can sometimes reduce accuracy when wrong
contexts are retrieved and passed to the generator. This is also mentioned in the
slides as a limitation: ``RAG can compromise results when wrong contexts are
passed''.

Overall, the best configuration combines DPR and reranking, showing that
improving retrieval quality improves end-to-end correctness. This is consistent
with DPR literature, where stronger retrieval precision is associated with
improved downstream QA accuracy \cite{karpukhin2020dense}, and with the RAG
motivation of reducing hallucination by providing evidence \cite{lewis2020retrieval}.

\section{Discussion}

The results show that adding a reranking step consistently improves retrieval
quality, as reflected by higher MRR values across all tested configurations. This
improvement is expected because the cross-encoder jointly processes the claim and
the passage, allowing it to model fine-grained semantic interactions that
bi-encoders cannot capture. As a result, the system is more likely to select
passages that are not only semantically related, but also directly relevant as
evidence for the claim.

The experiments also highlight that retrieval improves factual grounding but can
negatively affect performance when incorrect or misleading passages are
retrieved. While RAG encourages the language model to rely on explicit evidence
rather than internal assumptions, it also makes the model sensitive to retrieval
errors. When the retrieved context is incorrect, the model may follow that
evidence and produce the wrong label. This behavior is consistent with previous
observations that errors in the retrieval stage can propagate to the generation
stage and harm downstream performance \cite{lewis2020retrieval}. The oracle
results illustrate the upper bound of the system when retrieval is perfect.

The relatively strong performance of the baseline model suggests that modern
language models already contain a large amount of general scientific knowledge in
their parameters. This supports the view of language models as implicit knowledge
bases, capable of answering many factual questions without external information.
However, this internal knowledge is static and not guaranteed to be accurate or
complete, especially for specialized or less common claims. This motivates the
use of retrieval-based grounding to improve reliability and transparency.

Finally, the use of DistilBERT as the backbone for both the retriever and the
reranker enables efficient training and inference while maintaining strong
representational power \cite{sanh2020distilbert}. The main computational
trade-off lies in the reranking step: increasing the size of the candidate pool
improves ranking quality but also increases inference cost, since each candidate
must be jointly encoded with the claim.

\section{Conclusion}

We implemented and evaluated a RAG pipeline for scientific fact-checking on
SciFact. We compared baseline LLM-only, oracle with golden evidence, RAG with
DPR, and RAG with DPR plus a reranker. The results show that reranking improves
retrieval quality (higher MRR) and generally improves end-to-end accuracy by
providing better evidence to the generator. However, incorrect retrieval can
still harm results, highlighting the importance of reliable evidence selection.

Possible improvements (as also suggested in the slides) include: applying a
similarity threshold to limit context depth, training DPR and reranker for more
epochs, improving hard negative selection for retriever training, and comparing
different generator models.

\bibliography{../references}
\bibliographystyle{plainnat}

\appendix

\section{Implementation Details}

\subsection{Hardware and Software}

The experiments were conducted using Python with PyTorch. Main libraries include
Hugging Face Datasets, Sentence-Transformers, and Ollama for serving the LLM
locally.

\subsection{Reproducibility}

The training and evaluation code is implemented in Jupyter notebooks with fixed
random seeds and explicit hyperparameter settings for retriever and reranker
training.

\end{document}

